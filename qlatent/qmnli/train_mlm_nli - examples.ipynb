{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b19c18e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForSequenceClassification,\\\n",
    "DataCollatorForLanguageModeling, DataCollatorWithPadding, Trainer, TrainingArguments, AutoModel, EvalPrediction, AutoConfig\n",
    "from datasets import load_dataset, Dataset, Features, load_metric, DatasetDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf5a9040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: /home/amonfadi/.conda/envs/gpu_env2/bin/python\n",
      "CUDA_LAUNCH_BLOCKING set: False\n",
      "TORCH_USE_CUDA_DSA set: False\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "# Check Python executable path\n",
    "print(\"Python executable:\", sys.executable)\n",
    "\n",
    "# Check if CUDA_LAUNCH_BLOCKING is set\n",
    "cuda_launch_blocking = \"CUDA_LAUNCH_BLOCKING\" in os.environ\n",
    "print(\"CUDA_LAUNCH_BLOCKING set:\", cuda_launch_blocking)\n",
    "\n",
    "# Check if TORCH_USE_CUDA_DSA is set\n",
    "torch_use_cuda_dsa = \"TORCH_USE_CUDA_DSA\" in os.environ\n",
    "print(\"TORCH_USE_CUDA_DSA set:\", torch_use_cuda_dsa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5320933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0521b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "# Path to the JSONL input file\n",
    "input_jsonl_file = \"he-nli-test.jsonl\"\n",
    "\n",
    "# Path to the CSV output file\n",
    "output_csv_file = \"jsonl_to_csv.csv\"\n",
    "\n",
    "def convert_jsonl_to_csv(input_jsonl_file, output_csv_file):\n",
    "    # Open the JSONL input file for reading\n",
    "    with open(input_jsonl_file, \"r\", encoding=\"utf-8\") as jsonl_file:\n",
    "        # Open the CSV output file for writing\n",
    "        with open(output_csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "            # Create a CSV writer object\n",
    "            csv_writer = csv.writer(csv_file)\n",
    "\n",
    "            # Write the header row to the CSV file\n",
    "            csv_writer.writerow([\"premise\", \"hypothesis\", \"label\"])\n",
    "\n",
    "            # Read each line from the JSONL file\n",
    "            for line in jsonl_file:\n",
    "                # Parse the JSON object\n",
    "                data = json.loads(line)\n",
    "\n",
    "                # Extract relevant fields from the JSON object\n",
    "                premise = data.get(\"translation1\", \"\")\n",
    "                hypothesis = data.get(\"translation2\", \"\")\n",
    "                label_list = data.get(\"annotator_labels\", \"[]\")\n",
    "\n",
    "                # If label_list is a string representation of a list, parse it\n",
    "                if isinstance(label_list, str):\n",
    "                    label_list = eval(label_list)\n",
    "\n",
    "                # Get the first label if available\n",
    "                label = label_list[0] if label_list else \"\"\n",
    "\n",
    "                # Write the extracted data to the CSV file\n",
    "                csv_writer.writerow([premise, hypothesis, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df17900c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForSequenceClassification,\\\n",
    "DataCollatorForLanguageModeling, DataCollatorWithPadding, Trainer, TrainingArguments, AutoModel, EvalPrediction, AutoConfig\n",
    "from datasets import load_dataset, Dataset, Features, load_metric, DatasetDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "from typing import Union\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "    \n",
    "    \n",
    "class SaveCheckpointByEpochCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Callback to save the model and tokenizer at the end of each epoch during training.\n",
    "\n",
    "    This callback saves the model and tokenizer state to a specified directory after each training epoch,\n",
    "    allowing for periodic checkpoints of the training process.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_dir: str, tokenizer):\n",
    "        \"\"\"\n",
    "        Initialize the SaveCheckpointByEpochCallback.\n",
    "\n",
    "        Args:\n",
    "            output_dir (str): The directory where the checkpoints will be saved.\n",
    "            tokenizer: The tokenizer associated with the model being trained.\n",
    "        \"\"\"\n",
    "        self.output_dir = output_dir  # Set the directory to save the checkpoints\n",
    "        self.tokenizer = tokenizer  # Set the tokenizer to be saved with the model\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, model=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Save the model and tokenizer at the end of each epoch.\n",
    "\n",
    "        This method is called automatically by the Trainer at the end of each epoch.\n",
    "        It saves the model and tokenizer to a subdirectory named after the current epoch.\n",
    "\n",
    "        Args:\n",
    "            args: The training arguments.\n",
    "            state: The current state of the Trainer.\n",
    "            control: The current control object.\n",
    "            model: The model being trained.\n",
    "            **kwargs: Additional keyword arguments.\n",
    "        \"\"\"\n",
    "        epoch = state.epoch  # Get the current epoch number\n",
    "        checkpoint_dir = f\"{self.output_dir}/checkpoint-epoch-{int(epoch)}\"  # Define the checkpoint directory for the current epoch\n",
    "        model.save_pretrained(checkpoint_dir)  # Save the model to the checkpoint directory\n",
    "        self.tokenizer.save_pretrained(checkpoint_dir)  # Save the tokenizer to the checkpoint directory    \n",
    "\n",
    "\n",
    "class ModelTrainer:\n",
    "        \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def _set_nested_attribute(self, obj, attribute_string: str, value):\n",
    "        \"\"\"\n",
    "        Set the value of a nested attribute in an object.\n",
    "\n",
    "        This method sets the value of a nested attribute (e.g., \"layer1.layer2.weight\") in an object.\n",
    "\n",
    "        Args:\n",
    "            obj: The object containing the nested attribute.\n",
    "            attribute_string (str): A string representing the nested attribute (e.g., \"layer1.layer2.weight\").\n",
    "            value: The value to set for the specified nested attribute.\n",
    "        \"\"\"\n",
    "        attrs = attribute_string.split('.')  # Split the attribute string into individual attributes\n",
    "        current_obj = obj\n",
    "        # Traverse the attribute hierarchy except for the last attribute\n",
    "        for attr in attrs[:-1]:\n",
    "            current_obj = getattr(current_obj, attr)  # Get the nested object\n",
    "        setattr(current_obj, attrs[-1], value)  # Set the final attribute value\n",
    "\n",
    "    def _get_nested_attribute(self, obj, attribute_string: str):\n",
    "        \"\"\"\n",
    "        Get the value of a nested attribute from an object.\n",
    "\n",
    "        This method retrieves the value of a nested attribute (e.g., \"layer1.layer2.weight\") from an object.\n",
    "\n",
    "        Args:\n",
    "            obj: The object containing the nested attribute.\n",
    "            attribute_string (str): A string representing the nested attribute (e.g., \"layer1.layer2.weight\").\n",
    "\n",
    "        Returns:\n",
    "            The value of the specified nested attribute.\n",
    "        \"\"\"\n",
    "        attributes = attribute_string.split(\".\")  # Split the attribute string into individual attributes\n",
    "        layer_obj = obj\n",
    "        # Traverse the attribute hierarchy\n",
    "        for attribute_name in attributes:\n",
    "            layer_obj = getattr(layer_obj, attribute_name)  # Get the nested object\n",
    "        return layer_obj  # Return the final attribute value    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def init_head(self, uninitialized_head : AutoModelForMaskedLM, initialized_head : AutoModelForMaskedLM, layers_to_init : list[str]):\n",
    "        model_name = uninitialized_head.base_model.config._name_or_path   \n",
    "        print(f\"===================================Copying layers weights and biases to {model_name} model===========\")\n",
    "        # this is done to copy the whole layer and not just an attribute of it, for example, at first we get: \"vocab_transform.weight\", and I want to access the whole layer \"vocab_transform\"\n",
    "        layers_to_init = list(set([\".\".join(layer.split(\".\")[:-1]) for layer in layers_to_init]))\n",
    "        for init_layer_name in layers_to_init:\n",
    "            if \".\" in init_layer_name: # if there are iterative nested attributes, for example: lm_head.decoder\n",
    "                \n",
    "                layer_obj = self._get_nested_attribute(initialized_head, init_layer_name)\n",
    "                \n",
    "                # attributes = init_layer_name.split(\".\")\n",
    "                # layer_obj = initialized_head\n",
    "                # for attribute_name in attributes:\n",
    "                #     layer_obj = getattr(layer_obj, attribute_name)   \n",
    "                self._set_nested_attribute(uninitialized_head, init_layer_name, layer_obj)\n",
    "                \n",
    "            else:           \n",
    "                setattr(uninitialized_head, init_layer_name, getattr(initialized_head, init_layer_name))\n",
    "            print(f\"The {init_layer_name} layer was copied from the initialized head!\")            \n",
    "        print(\"===================================Done copying layers weights and biases===================================\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def _preprocess_logits_for_metrics_mlm(self, logits, labels):\n",
    "        if isinstance(logits, tuple):\n",
    "            # Depending on the model and config, logits may contain extra tensors,\n",
    "            # like past_key_values, but logits always come first\n",
    "            logits = logits[0]\n",
    "        return logits.argmax(dim=-1)\n",
    "\n",
    "\n",
    "    def _compute_metrics_mlm(self, eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        #predictions = logits.argmax(-1)\n",
    "        metric = load_metric(\"accuracy\")\n",
    "\n",
    "        predictions = predictions.reshape(-1)\n",
    "        labels = labels.reshape(-1)\n",
    "        # Convert predictions and labels to lists\n",
    "        mask = labels != -100\n",
    "        labels = labels[mask]\n",
    "        predictions = predictions[mask]\n",
    "\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "    \n",
    "    \n",
    "    def _compute_metrics_nli(self, p: EvalPrediction):\n",
    "        preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "        metric = load_metric(\"accuracy\")\n",
    "        result = metric.compute(predictions=preds, references=p.label_ids)\n",
    "        if len(result) > 1:\n",
    "            result[\"combined_score\"] = np.mean(list(result.values())).item()\n",
    "        return result\n",
    "\n",
    "    \n",
    "    def _train_mlm(self, model, tokenizer, dataset : Union[str, DatasetDict], num_samples_train, num_samples_validation, val_dataset, validate, batch_size, num_epochs, learning_rate, checkpoint_path, freeze_base, training_model_max_tokens):\n",
    "                \n",
    "#         class CustomTrainer(Trainer):\n",
    "#             def save_model(self, output_dir=None, _internal_call=False):\n",
    "#                 if output_dir is None:\n",
    "#                     output_dir = self.args.output_dir\n",
    "#                 super().save_model(output_dir, _internal_call)\n",
    "#                 if self.tokenizer is not None:\n",
    "#                     self.tokenizer.save_pretrained(output_dir)\n",
    "#                     print(f\"Tokenizer saved to {output_dir}\")        \n",
    "\n",
    "        # Tokenize the combined dataset\n",
    "        def preprocess_function(dataset):\n",
    "            return tokenizer(dataset['text'], truncation=True, padding=True, max_length=training_model_max_tokens)  \n",
    "        \n",
    "        if not isinstance(dataset, str) and not isinstance(dataset, DatasetDict):\n",
    "            raise TypeError(\"dataset must be of type 'str' or 'Dataset'\")\n",
    "        \n",
    "        if val_dataset is not None and not validate:\n",
    "            raise ValueError(\"If a validation dataset is provided then validate must be True!\")\n",
    "        \n",
    "        \n",
    "        if isinstance(dataset, str):\n",
    "            if dataset[-4:] != \".csv\":\n",
    "                raise ValueError(\"The dataset must be a path to a csv file.\")\n",
    "        \n",
    "        \n",
    "            sentences = []\n",
    "            with open(dataset, newline='', encoding='utf-8') as csvfile:\n",
    "                csv_reader = csv.reader(csvfile)\n",
    "                for row in csv_reader:\n",
    "                    if row[0]==\"\":\n",
    "                        raise ValueError(\"There is an empty row at the dataset!\")\n",
    "                    # Assuming each row contains only one value\n",
    "                    sentences.append(row[0])\n",
    "                    \n",
    "            #random.shuffle(sentences)\n",
    "            if num_samples_train:\n",
    "                training_set = sentences[:num_samples_train]\n",
    "            else:\n",
    "                training_set = sentences\n",
    "                \n",
    "            if val_dataset and validate:     \n",
    "                validation_set=[]\n",
    "                with open(val_dataset, newline='', encoding='utf-8') as csvfile:\n",
    "                    csv_reader = csv.reader(csvfile)\n",
    "                    for row in csv_reader:\n",
    "                        if row[0]==\"\":\n",
    "                            raise ValueError(\"There is an empty row at the dataset!\")\n",
    "                        # Assuming each row contains only one value\n",
    "                        validation_set.append(row[0])\n",
    "                            # Create Dataset objects for each split\n",
    "                            \n",
    "                train_dataset = Dataset.from_dict({\"text\": training_set})\n",
    "                validation_dataset = Dataset.from_dict({\"text\": validation_set})\n",
    "\n",
    "                dataset = DatasetDict({\"train\": train_dataset, \"validation\": validation_dataset})\n",
    "            elif validate:\n",
    "                # Split samples into training and validation sets\n",
    "                if num_samples_train:\n",
    "                    validation_set = sentences[num_samples_train:]\n",
    "                \n",
    "                else:\n",
    "                    raise TypeError(\"Since num_samples_train is not provided, the validation dataset would include samples from training, so please specify num_samples_train\")\n",
    "                    \n",
    "                # Create Dataset objects for each split\n",
    "                train_dataset = Dataset.from_dict({\"text\": training_set})\n",
    "                validation_dataset = Dataset.from_dict({\"text\": validation_set})\n",
    "\n",
    "                dataset = DatasetDict({\"train\": train_dataset, \"validation\": validation_dataset})\n",
    "                \n",
    "            else:\n",
    "                # Create Dataset objects for each split\n",
    "                train_dataset = Dataset.from_dict({\"text\": training_set})\n",
    "                dataset = DatasetDict({\"train\": train_dataset})\n",
    "        \n",
    "                      \n",
    "        tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "        if num_samples_train:\n",
    "            print(f\"Sampling {num_samples_train} training samples!\")\n",
    "            train_sampled_dataset = tokenized_dataset['train'].select(range(num_samples_train))\n",
    "        else:\n",
    "            print(f\"num_samples_train was not provided, using whole {len(tokenized_dataset['train'])} training samples!\")\n",
    "            train_sampled_dataset = tokenized_dataset['train']\n",
    "                    \n",
    "        if num_samples_validation and validate:\n",
    "            print(f\"Sampling {num_samples_validation} validation samples!\")\n",
    "            validation_sampled_dataset = tokenized_dataset['validation'].select(range(num_samples_validation))\n",
    "            \n",
    "        elif validate:\n",
    "            print(f\"num_samples_validation was not provided, using whole {len(tokenized_dataset['validation'])} validation samples!\")\n",
    "            validation_sampled_dataset = tokenized_dataset['validation']\n",
    "                \n",
    " \n",
    "#         # Sample the indices of the items\n",
    "#         train_sampled_indices = random.sample(range(len(tokenized_dataset['train'])), num_samples_train)\n",
    "#         # Create a new dataset with the sampled items\n",
    "#         train_sampled_dataset=tokenized_dataset['train'].select(train_sampled_indices)\n",
    "        \n",
    "          \n",
    "#         validation_sampled_indices = random.sample(range(len(tokenized_dataset['validation'])), num_samples_validation)\n",
    "#         validation_sampled_dataset=tokenized_dataset['validation'].select(validation_sampled_indices)\n",
    "\n",
    "\n",
    "\n",
    "        data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "\n",
    "        # Freeze/unfreeze base model\n",
    "        for param in model.base_model.parameters():\n",
    "            param.requires_grad = not freeze_base\n",
    "\n",
    "        if validate:\n",
    "            # Define training arguments\n",
    "            training_args = TrainingArguments(\n",
    "                per_device_train_batch_size=batch_size,\n",
    "                per_device_eval_batch_size=batch_size,\n",
    "                num_train_epochs=num_epochs,\n",
    "                learning_rate=learning_rate,\n",
    "                evaluation_strategy=\"epoch\",  # Log metrics at the end of each epoch\n",
    "                logging_dir=\"./mlm_training/logs/logging_mlm\",\n",
    "                output_dir=\"./mlm_training/output\", \n",
    "                overwrite_output_dir = True,\n",
    "                save_strategy=\"no\",\n",
    "                #save_strategy=\"epoch\",  # Save checkpoint at the end of each epoch\n",
    "            )\n",
    "\n",
    "            # Define Trainer\n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_sampled_dataset,\n",
    "                eval_dataset=validation_sampled_dataset,\n",
    "                data_collator=data_collator,\n",
    "                compute_metrics=self._compute_metrics_mlm,\n",
    "                preprocess_logits_for_metrics=self._preprocess_logits_for_metrics_mlm,\n",
    "                callbacks=[SaveCheckpointByEpochCallback(checkpoint_path, tokenizer)],\n",
    "            )\n",
    "        else:\n",
    "            # Define training arguments\n",
    "            training_args = TrainingArguments(\n",
    "                per_device_train_batch_size=batch_size,\n",
    "                num_train_epochs=num_epochs,\n",
    "                learning_rate=learning_rate,\n",
    "                logging_dir=\"./mlm_training/logs/logging_mlm\",  \n",
    "                output_dir=\"./mlm_training/output\", \n",
    "                overwrite_output_dir = True,\n",
    "                save_strategy=\"no\",\n",
    "                #save_strategy=\"epoch\",  # Save checkpoint at the end of each epoch\n",
    "            )\n",
    "\n",
    "            # Define Trainer\n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_sampled_dataset,\n",
    "                data_collator=data_collator,\n",
    "                compute_metrics=self._compute_metrics_mlm,\n",
    "                preprocess_logits_for_metrics=self._preprocess_logits_for_metrics_mlm,\n",
    "                callbacks=[SaveCheckpointByEpochCallback(checkpoint_path, tokenizer)],\n",
    "\n",
    "            )\n",
    "\n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "        return model\n",
    "\n",
    "    \n",
    "    \n",
    "    def _train_nli(self, model, tokenizer, dataset : Union[str, DatasetDict], num_samples_train, num_samples_validation, val_dataset, validate, batch_size, num_epochs, learning_rate, checkpoint_path, freeze_base, training_model_max_tokens):\n",
    "                  \n",
    "                  \n",
    "        # Tokenize the combined dataset\n",
    "        def preprocess_function(dataset):\n",
    "            return tokenizer(dataset['premise'], dataset['hypothesis'], padding=True, truncation=True, max_length=training_model_max_tokens)  \n",
    "        \n",
    "        \n",
    "        if not isinstance(dataset, str) and not isinstance(dataset, DatasetDict):\n",
    "            raise TypeError(\"dataset must be of type 'str' or 'Dataset'\")\n",
    "            \n",
    "        if val_dataset is not None and not validate:\n",
    "            raise ValueError(\"If a validation dataset is provided then validate must be True!\")\n",
    "        \n",
    "        if isinstance(dataset, str):                \n",
    "            if dataset[-4:] != \".csv\":\n",
    "                raise ValueError(\"The dataset must be a path to a csv file.\")\n",
    "        \n",
    "            training_premise=[]\n",
    "            training_hypothesis=[]\n",
    "            training_label=[]\n",
    "            label2_id = {'entailment': 0, 'neutral': 1, 'contradiction':2}\n",
    "            with open(dataset, newline='', encoding='utf-8') as csvfile:\n",
    "                csv_reader = csv.reader(csvfile)\n",
    "                next(csv_reader)  # Skip the header row\n",
    "                for row in csv_reader:\n",
    "                    training_premise.append(row[0])\n",
    "                    training_hypothesis.append(row[1])\n",
    "                    training_label.append(label2_id[row[2]])\n",
    "                    \n",
    "            if num_samples_train:\n",
    "                training_premise=training_premise[:num_samples_train]\n",
    "                training_hypothesis=training_hypothesis[:num_samples_train]\n",
    "                training_label=training_label[:num_samples_train]\n",
    "                    \n",
    "            if val_dataset and validate:     \n",
    "                validation_premise=[]\n",
    "                validation_hypothesis=[]\n",
    "                validation_label=[]\n",
    "                with open(val_dataset, newline='', encoding='utf-8') as csvfile:\n",
    "                    csv_reader = csv.reader(csvfile)\n",
    "                    next(csv_reader)  # Skip the header row\n",
    "                    for row in csv_reader:\n",
    "                        validation_premise.append(row[0])\n",
    "                        validation_hypothesis.append(row[1])\n",
    "                        validation_label.append(label2_id[row[2]])\n",
    "\n",
    "                # Create a dictionary with the data\n",
    "                training_set = {\n",
    "                    'premise': training_premise,\n",
    "                    'hypothesis': training_hypothesis,\n",
    "                    'label': training_label\n",
    "                }\n",
    "                \n",
    "                # Create a dictionary with the data\n",
    "                validation_set = {\n",
    "                    'premise': validation_premise,\n",
    "                    'hypothesis': validation_hypothesis,\n",
    "                    'label': validation_label\n",
    "                }\n",
    "\n",
    "                # Create Dataset objects for each split\n",
    "                train_dataset = Dataset.from_dict(training_set)\n",
    "                validation_dataset = Dataset.from_dict(validation_set)\n",
    "\n",
    "                dataset = DatasetDict({\"train\": train_dataset, \"validation_matched\": validation_dataset})\n",
    "                \n",
    "                \n",
    "                \n",
    "            elif validate:  \n",
    "                if num_samples_train:                             \n",
    "                    validation_premise = training_premise[num_samples_train:]\n",
    "                    validation_hypothesis = training_hypothesis[num_samples_train:]\n",
    "                    validation_label = training_label[num_samples_train:]\n",
    "                \n",
    "                else:\n",
    "                    raise TypeError(\"Since num_samples_train is not provided, the validation dataset would include samples from training, so please specify num_samples_train\")\n",
    "                # Create a dictionary with the data\n",
    "                training_set = {\n",
    "                    'premise': training_premise,\n",
    "                    'hypothesis': training_hypothesis,\n",
    "                    'label': training_label\n",
    "                }\n",
    "                \n",
    "                # Create a dictionary with the data\n",
    "                validation_set = {\n",
    "                    'premise': validation_premise,\n",
    "                    'hypothesis': validation_hypothesis,\n",
    "                    'label': validation_label\n",
    "                }\n",
    "\n",
    "                # Create Dataset objects for each split\n",
    "                train_dataset = Dataset.from_dict(training_set)\n",
    "                validation_dataset = Dataset.from_dict(validation_set)\n",
    "\n",
    "                dataset = DatasetDict({\"train\": train_dataset, \"validation_matched\": validation_dataset})\n",
    "                \n",
    "            else:\n",
    "                training_set = {\n",
    "                    'premise': training_premise,\n",
    "                    'hypothesis': training_hypothesis,\n",
    "                    'label': training_label\n",
    "                }\n",
    "                # Create Dataset objects for each split\n",
    "                train_dataset = Dataset.from_dict({\"features\": training_set})\n",
    "                dataset = DatasetDict({\"train\": train_dataset})\n",
    "        \n",
    "\n",
    "            \n",
    "        tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "        if num_samples_train:\n",
    "            print(f\"Sampling {num_samples_train} training samples!\")\n",
    "            train_sampled_dataset = tokenized_dataset['train'].select(range(num_samples_train))\n",
    "        else:\n",
    "            print(f\"num_samples_train was not provided, using whole {len(tokenized_dataset['train'])} training samples!\")\n",
    "            train_sampled_dataset = tokenized_dataset['train']\n",
    "\n",
    "        if num_samples_validation and validate:\n",
    "            print(f\"Sampling {num_samples_validation} validation samples!\")\n",
    "            validation_sampled_dataset = tokenized_dataset['validation_matched'].select(range(num_samples_validation))\n",
    "\n",
    "        elif validate:\n",
    "            print(f\"num_samples_validation was not provided, using whole {len(tokenized_dataset['validation_matched'])} validation samples!\")\n",
    "            validation_sampled_dataset = tokenized_dataset['validation_matched']            \n",
    "                  \n",
    "\n",
    "        \n",
    "#         train_random_indices = random.sample(range(len(tokenized_dataset['train'])), num_samples_train)\n",
    "#         train_sampled_dataset = tokenized_dataset['train'].select(train_random_indices)        \n",
    "        \n",
    "#         validation_random_indices = random.sample(range(len(tokenized_dataset['validation_matched'])), num_samples_validation)\n",
    "#         validation_sampled_dataset = tokenized_dataset['validation_matched'].select(validation_random_indices)\n",
    "        \n",
    "        \n",
    "        \n",
    "        data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    \n",
    "        # Freeze/unfreeze base model\n",
    "        for param in model.base_model.parameters():\n",
    "            param.requires_grad = not freeze_base\n",
    "        \n",
    "        \n",
    "        if validate:\n",
    "            # Define training arguments\n",
    "            training_args = TrainingArguments(\n",
    "                per_device_train_batch_size=batch_size,\n",
    "                per_device_eval_batch_size=batch_size,\n",
    "                num_train_epochs=num_epochs,\n",
    "                learning_rate=learning_rate,\n",
    "                evaluation_strategy=\"epoch\",  # Log metrics at the end of each epoch\n",
    "                logging_dir=\"./nli_training/logs/logging_nli\",  \n",
    "                output_dir=\"./nli_training/output_benevolent/\",\n",
    "                #save_strategy=\"epoch\",  # Save checkpoint at the end of each epoch\n",
    "                overwrite_output_dir = True,\n",
    "                save_strategy=\"no\",\n",
    "\n",
    "            )\n",
    "\n",
    "            # Define Trainer\n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_sampled_dataset,\n",
    "                eval_dataset=validation_sampled_dataset,\n",
    "                data_collator=data_collator,\n",
    "                compute_metrics=self._compute_metrics_mlm,\n",
    "                preprocess_logits_for_metrics=self._preprocess_logits_for_metrics_mlm,\n",
    "                callbacks=[SaveCheckpointByEpochCallback(checkpoint_path, tokenizer)],\n",
    "            )\n",
    "        else:\n",
    "            # Define training arguments\n",
    "            training_args = TrainingArguments(\n",
    "                per_device_train_batch_size=batch_size,\n",
    "                num_train_epochs=num_epochs,\n",
    "                learning_rate=learning_rate,\n",
    "                logging_dir=\"./nli_training/logs/logging_nli\",  \n",
    "                output_dir=\"./nli_training/output\",\n",
    "                #save_strategy=\"epoch\",  # Save checkpoint at the end of each epoch\n",
    "                overwrite_output_dir = True,\n",
    "                save_strategy=\"no\",\n",
    "\n",
    "            )\n",
    "            \n",
    "\n",
    "            # Define Trainer\n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_sampled_dataset,\n",
    "                data_collator=data_collator,\n",
    "                compute_metrics=self._compute_metrics_mlm,\n",
    "                preprocess_logits_for_metrics=self._preprocess_logits_for_metrics_mlm,\n",
    "                callbacks=[SaveCheckpointByEpochCallback(checkpoint_path, tokenizer)],\n",
    "            )\n",
    "    \n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def get_non_base_layers(self, model):\n",
    "        \n",
    "        all_layers = list(model.state_dict().keys())\n",
    "        base_layers = list(model.base_model.state_dict().keys())\n",
    "        head_layers=[]\n",
    "        for layer in all_layers:\n",
    "            if \".\".join(layer.split(\".\")[1:]) not in base_layers: # when looping over the layers of the base model we want to remove the prefix of the layer which is the name of the model, hence the \".\".join(layer.split(\".\")[1:])\n",
    "                head_layers.append(layer)\n",
    "                \n",
    "        return head_layers\n",
    "    \n",
    "    \n",
    "    def attach_head_to_model(self, model, model_identifier : str, head):       \n",
    "        setattr(head, model_identifier, getattr(model,model_identifier))\n",
    "    \n",
    "        \n",
    "\n",
    "    def train_head(self, model, tokenizer, dataset, nli_head=False, mlm_head=False, model_to_copy_weights_from=None, num_samples_train=None, num_samples_validation=None,val_dataset=None,validate=True,training_model_max_tokens=512, batch_size=16, num_epochs=10, learning_rate=2e-5, freeze_base = False, copy_weights=False, checkpoint_path=None):\n",
    "        model_name = model.base_model.config._name_or_path         \n",
    "        \n",
    "        if  (not nli_head and not mlm_head) or (nli_head and mlm_head): # if both false or both true\n",
    "            raise ValueError(\"You must have one head (nli_head or mlm_head) set to True at a time.\")\n",
    "            \n",
    "\n",
    "        if copy_weights:\n",
    "            \n",
    "            if not model_to_copy_weights_from:\n",
    "                raise ValueError(\"Please pass in a model (model_to_copy_weights_from=?) to load the initialized layers from!\")\n",
    "                \n",
    "            \n",
    "            get_initialized_layers = self.get_non_base_layers(model_to_copy_weights_from)\n",
    "            get_uninitialized_layers = self.get_non_base_layers(model)\n",
    "            if sorted(get_uninitialized_layers)!=sorted(get_initialized_layers):\n",
    "                raise ValueError(f\"Models architecture are not equal, make sure that {model_to_copy_weights_from.base_model.config._name_or_path} head layers are the same as {model_name}'s\")\n",
    "            self.init_head(model, model_to_copy_weights_from, get_uninitialized_layers)\n",
    "\n",
    "        \n",
    "        if nli_head:\n",
    "            print(f\"Detected {model_name} with an NLI head...\")\n",
    "            if not checkpoint_path:\n",
    "                checkpoint_path = \"./nli_training_checkpoint\"\n",
    "            self._train_nli(model, tokenizer, dataset, num_samples_train, num_samples_validation,val_dataset, validate, batch_size, num_epochs, learning_rate, checkpoint_path, freeze_base, training_model_max_tokens)\n",
    "        elif mlm_head:\n",
    "            print(f\"Detected {model_name} with an MLM head...\")\n",
    "            if not checkpoint_path:\n",
    "                checkpoint_path = \"./mlm_training_checkpoint\"\n",
    "            self._train_mlm(model, tokenizer, dataset, num_samples_train, num_samples_validation,val_dataset, validate, batch_size, num_epochs, learning_rate, checkpoint_path, freeze_base, training_model_max_tokens)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a050dd2",
   "metadata": {},
   "source": [
    "### Example of an MNLI head loaded from an MNLI model trained with csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5290887b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected typeform/distilbert-base-uncased-mnli with an NLI head...\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0067157745361328125,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 4000,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4014752d0805469f8c927a709cda3b96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 4000 training samples!\n",
      "Sampling 1000 validation samples!\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfadi_\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/sise/home/amonfadi/Domain adaptation/wandb/run-20240602_155143-t6ge64ry</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fadi_/huggingface/runs/t6ge64ry\" target=\"_blank\">proud-planet-168</a></strong> to <a href=\"https://wandb.ai/fadi_/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/fadi_/huggingface\" target=\"_blank\">https://wandb.ai/fadi_/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/fadi_/huggingface/runs/t6ge64ry\" target=\"_blank\">https://wandb.ai/fadi_/huggingface/runs/t6ge64ry</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 08:06, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.071400</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.071400</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.922700</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.922700</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.790300</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.790300</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.633600</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.633600</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.534000</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example usage\n",
    "base_model_name = \"typeform/distilbert-base-uncased-mnli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained(base_model_name)\n",
    "dataset = \"./jsonl_to_csv.csv\"\n",
    "\n",
    "\n",
    "trainer = ModelTrainer()\n",
    "trainer.train_head(nli_model, tokenizer, nli_head=True, dataset=dataset, num_samples_train=4000, num_samples_validation=1000, copy_weights=False, freeze_base = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448ce62a",
   "metadata": {},
   "source": [
    "### Example of an MLM head loaded from an MLM model trained with dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b608aa99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected distilbert/distilbert-base-uncased with an MLM head...\n",
      "Sampling 10000 training samples!\n",
      "Sampling 2000 validation samples!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 44:56, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.120700</td>\n",
       "      <td>1.847978</td>\n",
       "      <td>0.625923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.986300</td>\n",
       "      <td>1.802680</td>\n",
       "      <td>0.640540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.904500</td>\n",
       "      <td>1.832532</td>\n",
       "      <td>0.627107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.863400</td>\n",
       "      <td>1.858084</td>\n",
       "      <td>0.628424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.821900</td>\n",
       "      <td>1.836857</td>\n",
       "      <td>0.628479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.781400</td>\n",
       "      <td>1.832940</td>\n",
       "      <td>0.631115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.766900</td>\n",
       "      <td>1.829467</td>\n",
       "      <td>0.629548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.737700</td>\n",
       "      <td>1.794244</td>\n",
       "      <td>0.635866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.708800</td>\n",
       "      <td>1.794745</td>\n",
       "      <td>0.635460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.717200</td>\n",
       "      <td>1.827078</td>\n",
       "      <td>0.634092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "base_model_name = \"distilbert/distilbert-base-uncased\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "mlm_model = AutoModelForMaskedLM.from_pretrained(base_model_name)\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "#dataset = \"../AMI/datasets/ami_hostility_towards_men.csv\"\n",
    "trainer = ModelTrainer()\n",
    "\n",
    "trainer.train_head(model=mlm_model, tokenizer=tokenizer,dataset=dataset,freeze_base=False, mlm_head=True, num_samples_train=10000, num_samples_validation=2000,validate=True, batch_size=16, training_model_max_tokens=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfab524c",
   "metadata": {},
   "source": [
    "### Example of an MLM head loaded from an MLM model trained with csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c89f8d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected distilbert/distilbert-base-uncased with an MLM head...\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.014665365219116211,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 100,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1776bcef3fbd4a02a5c3896a7921d476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_samples_train was not provided, using whole 100 training samples!\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfadi_\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/sise/home/amonfadi/Domain adaptation/wandb/run-20240530_090954-9l23ljmi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fadi_/huggingface/runs/9l23ljmi\" target=\"_blank\">denim-firebrand-162</a></strong> to <a href=\"https://wandb.ai/fadi_/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/fadi_/huggingface\" target=\"_blank\">https://wandb.ai/fadi_/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/fadi_/huggingface/runs/9l23ljmi\" target=\"_blank\">https://wandb.ai/fadi_/huggingface/runs/9l23ljmi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 00:46, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example usage\n",
    "base_model_name = \"distilbert/distilbert-base-uncased\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "mlm_model = AutoModelForMaskedLM.from_pretrained(base_model_name)\n",
    "#dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "dataset = \"../AMI/datasets/ami_hostility_towards_men.csv\"\n",
    "trainer = ModelTrainer()\n",
    "\n",
    "trainer.train_head(model=mlm_model, tokenizer=tokenizer,dataset=dataset,freeze_base=False, mlm_head=True,validate=False, batch_size=4, training_model_max_tokens=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956f5827",
   "metadata": {},
   "source": [
    "### Example of an MLM head loaded from an NLI model with weights and biases initialized randomly, trained with dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b62e1807",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Some weights of the model checkpoint at typeform/distilbert-base-uncased-mnli were not used when initializing DistilBertForMaskedLM: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "- This IS expected if you are initializing DistilBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForMaskedLM were not initialized from the model checkpoint at typeform/distilbert-base-uncased-mnli and are newly initialized: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected typeform/distilbert-base-uncased-mnli with an MLM head...\n",
      "Sampling 10000 training samples!\n",
      "Sampling 2000 validation samples!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 46:52, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.981700</td>\n",
       "      <td>4.641768</td>\n",
       "      <td>0.326060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.549800</td>\n",
       "      <td>3.976776</td>\n",
       "      <td>0.390801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.018600</td>\n",
       "      <td>3.655701</td>\n",
       "      <td>0.426218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.520600</td>\n",
       "      <td>3.470596</td>\n",
       "      <td>0.454778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.347500</td>\n",
       "      <td>3.303706</td>\n",
       "      <td>0.472434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.206300</td>\n",
       "      <td>3.212016</td>\n",
       "      <td>0.485991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.131200</td>\n",
       "      <td>3.134902</td>\n",
       "      <td>0.495727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.019500</td>\n",
       "      <td>3.025861</td>\n",
       "      <td>0.504404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.967100</td>\n",
       "      <td>3.025415</td>\n",
       "      <td>0.506924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.944400</td>\n",
       "      <td>3.044332</td>\n",
       "      <td>0.510294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_276180/1595216971.py:73: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library  Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"accuracy\")\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "base_model_name = \"typeform/distilbert-base-uncased-mnli\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "mlm_model = AutoModelForMaskedLM.from_pretrained(base_model_name)\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "#dataset = \"../AMI/datasets/ami_hostility_towards_men.csv\"\n",
    "trainer = ModelTrainer()\n",
    "\n",
    "trainer.train_head(model=mlm_model, tokenizer=tokenizer,dataset=dataset, mlm_head=True, copy_weights=False,num_samples_train=10000, num_samples_validation=2000, batch_size=16, training_model_max_tokens=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b8af4d",
   "metadata": {},
   "source": [
    "### Example of an MLM head loaded from an NLI model with weights and biases initialized randomly, trained with csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96de4e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Some weights of the model checkpoint at typeform/distilbert-base-uncased-mnli were not used when initializing DistilBertForMaskedLM: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "- This IS expected if you are initializing DistilBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForMaskedLM were not initialized from the model checkpoint at typeform/distilbert-base-uncased-mnli and are newly initialized: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected typeform/distilbert-base-uncased-mnli with an MLM head...\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012900829315185547,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 100,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc4f72b42660498598ef4216aa08d7c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_samples_train was not provided, using whole 100 training samples!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 00:49, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model_name = \"typeform/distilbert-base-uncased-mnli\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "mlm_model = AutoModelForMaskedLM.from_pretrained(base_model_name)\n",
    "#dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "dataset = \"../AMI/datasets/ami_hostility_towards_men.csv\"\n",
    "trainer = ModelTrainer()\n",
    "\n",
    "trainer.train_head(model=mlm_model, tokenizer=tokenizer,dataset=dataset, mlm_head=True, validate=False, batch_size=4, training_model_max_tokens=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e290e45",
   "metadata": {},
   "source": [
    "### Example of an MLM head loaded from an NLI model with copied weights and biases from a trained MLM head, trained on dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e17d7388",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Some weights of the model checkpoint at typeform/distilbert-base-uncased-mnli were not used when initializing DistilBertForMaskedLM: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "- This IS expected if you are initializing DistilBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForMaskedLM were not initialized from the model checkpoint at typeform/distilbert-base-uncased-mnli and are newly initialized: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================Copying layers weights and biases to typeform/distilbert-base-uncased-mnli model===========\n",
      "The vocab_transform layer was copied from the initialized head!\n",
      "The vocab_projector layer was copied from the initialized head!\n",
      "The vocab_layer_norm layer was copied from the initialized head!\n",
      "===================================Done copying layers weights and biases===================================\n",
      "Detected typeform/distilbert-base-uncased-mnli with an MLM head...\n",
      "Sampling 10000 training samples!\n",
      "Sampling 2000 validation samples!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 47:44, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.565300</td>\n",
       "      <td>2.080645</td>\n",
       "      <td>0.591742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.232100</td>\n",
       "      <td>1.982854</td>\n",
       "      <td>0.611504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.112600</td>\n",
       "      <td>1.991851</td>\n",
       "      <td>0.602558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.031600</td>\n",
       "      <td>1.996808</td>\n",
       "      <td>0.608984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.982900</td>\n",
       "      <td>1.966853</td>\n",
       "      <td>0.608812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.930600</td>\n",
       "      <td>1.968479</td>\n",
       "      <td>0.609883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.909900</td>\n",
       "      <td>1.959997</td>\n",
       "      <td>0.610469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.875500</td>\n",
       "      <td>1.914934</td>\n",
       "      <td>0.619345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.844800</td>\n",
       "      <td>1.925399</td>\n",
       "      <td>0.616508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.852100</td>\n",
       "      <td>1.952806</td>\n",
       "      <td>0.614754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "base_model_name = \"typeform/distilbert-base-uncased-mnli\" \n",
    "mlm_initialized_head = AutoModelForMaskedLM.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "# Example usage\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "mlm_model = AutoModelForMaskedLM.from_pretrained(base_model_name)\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "#dataset = \"../AMI/datasets/ami_hostility_towards_men.csv\"\n",
    "trainer = ModelTrainer()\n",
    "\n",
    "trainer.train_head(model=mlm_model, tokenizer=tokenizer,dataset=dataset, mlm_head=True, model_to_copy_weights_from=mlm_initialized_head, copy_weights=True,num_samples_train=10000, num_samples_validation=2000, batch_size=16, training_model_max_tokens=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b1720f",
   "metadata": {},
   "source": [
    "### Example of an MLM head loaded from an NLI model with copied weights and biases from a trained MLM head, trained on csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "764d7cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Some weights of the model checkpoint at typeform/distilbert-base-uncased-mnli were not used when initializing DistilBertForMaskedLM: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "- This IS expected if you are initializing DistilBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForMaskedLM were not initialized from the model checkpoint at typeform/distilbert-base-uncased-mnli and are newly initialized: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================Copying layers weights and biases to typeform/distilbert-base-uncased-mnli model===========\n",
      "The vocab_transform layer was copied from the initialized head!\n",
      "The vocab_projector layer was copied from the initialized head!\n",
      "The vocab_layer_norm layer was copied from the initialized head!\n",
      "===================================Done copying layers weights and biases===================================\n",
      "Detected typeform/distilbert-base-uncased-mnli with an MLM head...\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03440999984741211,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 100,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f089879e7ce248cd9d2193106631e2ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_samples_train was not provided, using whole 100 training samples!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 00:58, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example usage\n",
    "base_model_name = \"typeform/distilbert-base-uncased-mnli\" \n",
    "mlm_initialized_head = AutoModelForMaskedLM.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "# Example usage\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "mlm_model = AutoModelForMaskedLM.from_pretrained(base_model_name)\n",
    "#dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "dataset = \"../AMI/datasets/ami_hostility_towards_men.csv\"\n",
    "trainer = ModelTrainer()\n",
    "\n",
    "trainer.train_head(model=mlm_model, tokenizer=tokenizer,dataset=dataset, mlm_head=True, model_to_copy_weights_from=mlm_initialized_head, copy_weights=True,validate=False, batch_size=4, training_model_max_tokens=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c33f281",
   "metadata": {},
   "source": [
    "### Example of an NLI head fine tuned with NLI model, trained on dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8be464c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected typeform/distilbert-base-uncased-mnli with an NLI head...\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009722709655761719,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 9815,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25ff60f5fdb649928a8c6a1e5466b8b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9815 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 10000 training samples!\n",
      "Sampling 2000 validation samples!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 16:21, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.179900</td>\n",
       "      <td>0.929243</td>\n",
       "      <td>0.821500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.121100</td>\n",
       "      <td>1.063527</td>\n",
       "      <td>0.805500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.090700</td>\n",
       "      <td>1.253619</td>\n",
       "      <td>0.812000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.039200</td>\n",
       "      <td>1.351176</td>\n",
       "      <td>0.815500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.025300</td>\n",
       "      <td>1.431156</td>\n",
       "      <td>0.817000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>1.482971</td>\n",
       "      <td>0.822000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>1.523677</td>\n",
       "      <td>0.821500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.016600</td>\n",
       "      <td>1.564941</td>\n",
       "      <td>0.823000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>1.591428</td>\n",
       "      <td>0.824000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>1.601259</td>\n",
       "      <td>0.821000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "base_model_name = \"typeform/distilbert-base-uncased-mnli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained(base_model_name)\n",
    "dataset = load_dataset('multi_nli')\n",
    "\n",
    "\n",
    "trainer = ModelTrainer()\n",
    "trainer.train_head(nli_model, tokenizer, nli_head=True, dataset=dataset, num_samples_train=10000, num_samples_validation=2000, copy_weights=False, freeze_base = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5109e5",
   "metadata": {},
   "source": [
    "### Example of an NLI head loaded from an MLM model with weights and biases initialized randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "542a80e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert/distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected distilbert/distilbert-base-uncased with an NLI head...\n",
      "Sampling 10000 training samples!\n",
      "Sampling 2000 validation samples!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 16:01, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.919200</td>\n",
       "      <td>0.772360</td>\n",
       "      <td>0.664000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.678300</td>\n",
       "      <td>0.752366</td>\n",
       "      <td>0.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.506700</td>\n",
       "      <td>0.811651</td>\n",
       "      <td>0.704000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.250200</td>\n",
       "      <td>1.052800</td>\n",
       "      <td>0.685500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.150400</td>\n",
       "      <td>1.327794</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.125500</td>\n",
       "      <td>1.646691</td>\n",
       "      <td>0.681500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.092700</td>\n",
       "      <td>1.760503</td>\n",
       "      <td>0.702500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.059900</td>\n",
       "      <td>1.821774</td>\n",
       "      <td>0.703500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.037700</td>\n",
       "      <td>1.903767</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.031800</td>\n",
       "      <td>1.926256</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "config = AutoConfig.from_pretrained(\"distilbert/distilbert-base-uncased\", num_labels = 3)\n",
    "base_model_name = \"distilbert/distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained(base_model_name , config=config)\n",
    "trainer = ModelTrainer()\n",
    "dataset = load_dataset('multi_nli')\n",
    "\n",
    "\n",
    "trainer.train_head(nli_model, tokenizer,dataset=dataset, nli_head=True, num_samples_train=10000, num_samples_validation=2000, copy_weights=False, freeze_base = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c62241",
   "metadata": {},
   "source": [
    "### Example of an NLI head loaded from MLM model with copied weights and biases from a trained NLI head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0aa65d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert/distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================Copying layers weights and biases to distilbert/distilbert-base-uncased model===========\n",
      "The pre_classifier layer was copied from the initialized head!\n",
      "The classifier layer was copied from the initialized head!\n",
      "===================================Done copying layers weights and biases===================================\n",
      "Detected distilbert/distilbert-base-uncased with an NLI head...\n",
      "Sampling 10000 training samples!\n",
      "Sampling 2000 validation samples!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 16:23, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.859900</td>\n",
       "      <td>0.727506</td>\n",
       "      <td>0.683500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.628200</td>\n",
       "      <td>0.756342</td>\n",
       "      <td>0.692000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.895567</td>\n",
       "      <td>0.698000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.156263</td>\n",
       "      <td>0.700500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.104800</td>\n",
       "      <td>1.516546</td>\n",
       "      <td>0.706000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.080200</td>\n",
       "      <td>1.767782</td>\n",
       "      <td>0.709500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>1.955309</td>\n",
       "      <td>0.711500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.024500</td>\n",
       "      <td>2.112643</td>\n",
       "      <td>0.711500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>2.162693</td>\n",
       "      <td>0.716500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.017400</td>\n",
       "      <td>2.167689</td>\n",
       "      <td>0.717000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/amonfadi/.conda/envs/gpu_env2/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "config = AutoConfig.from_pretrained(\"distilbert/distilbert-base-uncased\", num_labels = 3)\n",
    "base_model_name = \"distilbert/distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained(base_model_name , config=config)\n",
    "nli_initialized_head = AutoModelForSequenceClassification.from_pretrained(\"typeform/distilbert-base-uncased-mnli\")\n",
    "trainer = ModelTrainer()\n",
    "dataset = load_dataset('multi_nli')\n",
    "\n",
    "trainer.train_head(nli_model, tokenizer,dataset=dataset, nli_head=True, model_to_copy_weights_from=nli_initialized_head, num_samples_train=10000, num_samples_validation=2000, copy_weights=True, freeze_base = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767ca3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mlm(model, tokenizer, sentence, device=\"cpu\"):  # Pass device as an argument\n",
    "    # Tokenize the input sentence\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "    # Move inputs to the specified device\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}  # Move all tensors to device\n",
    "\n",
    "    # Find the index of the [MASK] token in the tokenized sequence\n",
    "    mask_index = inputs['input_ids'][0].tolist().index(tokenizer.mask_token_id)\n",
    "\n",
    "    # Generate predictions\n",
    "    with torch.no_grad():\n",
    "        # Move the model to the specified device\n",
    "        model.to(device)  # Move the model to device\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Get the predicted logits for the masked token\n",
    "    masked_token_logits = outputs.logits[0, mask_index].cpu().numpy()\n",
    "\n",
    "    # Convert logits to probabilities\n",
    "    masked_token_probs = torch.softmax(torch.tensor(masked_token_logits), dim=-1).numpy()\n",
    "\n",
    "    # Get the predicted token ID with highest probability\n",
    "    predicted_token_id = int(masked_token_probs.argmax())\n",
    "\n",
    "    # Get the predicted token\n",
    "    predicted_token = tokenizer.convert_ids_to_tokens([predicted_token_id])[0]\n",
    "\n",
    "    return predicted_token\n",
    "\n",
    "# Example sentence to test\n",
    "sentence = \"The quick brown [MASK] jumps over the lazy dog.\"\n",
    "\n",
    "# Test the MLM model\n",
    "predicted_token = test_mlm(mlm_model, tokenizer, sentence)  # Specify device as \"cpu\"\n",
    "\n",
    "print(\"Predicted token:\", predicted_token)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
